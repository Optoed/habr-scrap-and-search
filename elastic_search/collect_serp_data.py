import json
import pandas as pd

from elastic_search.habr_search import HabrSearchEngine

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
test_queries_config = [
    # Simple –ø–æ–∏—Å–∫ - –±–µ–∑ –±—É—Å—Ç–∏–Ω–≥–∞, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ö–æ—Ç—è –±—ã 67% —Å–ª–æ–≤
    {'query': 'python –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'type': 'simple'},
    {'query': 'docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è', 'type': 'simple'},
    {'query': '–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 'type': 'simple'},
    {'query': '–≤–µ–± —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ frontend', 'type': 'simple'},
    {'query': '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö SQL', 'type': 'simple'},
    {'query': '–ø–∞–π—Ç–æ–Ω –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 'type': 'simple'},
    {'query': 'javascript —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏', 'type': 'simple'},
    {'query': 'golang –Ω–æ–≤–∏—á–∫—É', 'type': 'simple'},
    {'query': '–º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', 'type': 'simple'},
    {'query': '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–≥—Ä unity', 'type': 'simple'},
]


class SERPCollector:
    def __init__(self, enable_spell_check=True):
        # –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å
        self.search_engine = HabrSearchEngine(enable_spell_check)

    def collect_serp_data(self, queries_config, output_json="serp_data.json", output_xlsx="serp_data.xlsx"):
        """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ JSON –∏ XLSX"""

        all_serp_data = {}
        all_articles = []

        for i, config in enumerate(queries_config, 1):
            query = config['query']
            search_type = config['type']

            print(f"\n[{i}/{len(queries_config)}] –ó–∞–ø—Ä–æ—Å: '{query}' (—Ç–∏–ø: {search_type})")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ search_articles –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            results = self.search_engine.search_articles(query, size=10, search_type=search_type)

            if not results or 'hits' not in results:
                print(f"  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
                continue

            hits = results['hits']['hits']
            query_results = []

            for rank, hit in enumerate(hits, 1):
                source = hit['_source']

                article_data = {
                    'relevance': None,  # –ü–ï–†–í–´–ô –°–¢–û–õ–ë–ï–¶ - –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏
                    'query_id': i,
                    'query_text': query,
                    'search_type': search_type,
                    'rank': rank,
                    'score': float(hit['_score']),
                    'title': source['title'],
                    'url': source['url'],
                    'author': source.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω'),
                    'date': source.get('date', ''),
                    'hubs': ', '.join(source.get('hubs', [])),
                    'tags': ', '.join(source.get('tags', [])),
                }

                query_results.append(article_data)
                all_articles.append(article_data)

                print(f"  {rank}. [{hit['_score']:.2f}] {source['title'][:70]}...")

            all_serp_data[f"{query}_{search_type}"] = {
                'search_type': search_type,
                'total_found': results['hits']['total']['value'],
                'articles_fetched': len(hits),
                'results': query_results
            }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(all_serp_data, f, ensure_ascii=False, indent=2, default=str)
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {output_json}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ XLSX
        if all_articles:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–æ–ª–æ–Ω–æ–∫
            columns_order = [
                'relevance',  # –ü–ï–†–í–´–ô –°–¢–û–õ–ë–ï–¶
                'query_id',
                'query_text',
                'search_type',
                'rank',
                'score',
                'title',
                'url',
                'author',
                'date',
                'hubs',
                'tags'
            ]

            df = pd.DataFrame(all_articles)
            df = df[columns_order]  # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            with pd.ExcelWriter(output_xlsx, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='SERP_Data', index=False)

            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ XLSX: {output_xlsx}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_articles = len(all_articles)
            unique_queries = len(queries_config)
            print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {unique_queries}")
            print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total_articles}")
            print(f"   –°—Ç–∞—Ç–µ–π –Ω–∞ –∑–∞–ø—Ä–æ—Å: {total_articles / unique_queries:.1f}")

        return all_serp_data


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–±–æ—Ä–∞ SERP –¥–∞–Ω–Ω—ã—Ö"""
    try:
        print("üîç –°–±–æ—Ä SERP –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 10 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print("=" * 60)
        print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤:")
        print("  10 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å SIMPLE –ø–æ–∏—Å–∫–æ–º (–º–∏–Ω–∏–º—É–º 67% —Å–ª–æ–≤)")
        print("=" * 60)
        print("–í—Å–µ–≥–æ –±—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω–æ: 10 –∑–∞–ø—Ä–æ—Å–æ–≤ √ó 10 —Å—Ç–∞—Ç–µ–π = 100 —Å—Ç–∞—Ç–µ–π")
        print("=" * 60)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ç–æ—Ä
        collector = SERPCollector()

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        serp_data = collector.collect_serp_data(
            queries_config=test_queries_config,
            output_json="habr_serp_data.json",
            output_xlsx="habr_serp_data.xlsx"
        )

        print("\n–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print("\n–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print("  - habr_serp_data.json - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ SERP")
        print("  - habr_serp_data.xlsx - —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (Excel)")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if serp_data:
            total_queries = len(serp_data)
            total_articles = sum(len(data['results']) for data in serp_data.values())

            print(f"\n–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"  –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_queries}/10")
            print(f"  –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {total_articles}")

            print("\n–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
            print("  1. –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª habr_serp_data.xlsx –≤ Excel")
            print("  2. –í —Å—Ç–æ–ª–±—Ü–µ 'relevance' (–ü–ï–†–í–´–ô –°–¢–û–õ–ë–ï–¶) –ø–æ—Å—Ç–∞–≤—å—Ç–µ:")
            print("     - 1 –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            print("     - 0 –¥–ª—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            print("  3. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ñ–∞–π–ª –ø–æ—Å–ª–µ —Ä–∞–∑–º–µ—Ç–∫–∏")

    except Exception as e:
        print(f"\n–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("  - –ó–∞–ø—É—â–µ–Ω –ª–∏ Elasticsearch –Ω–∞ localhost:9200")
        print("  - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏–º–ø–æ—Ä—Ç–∞ –∫–ª–∞—Å—Å–∞ HabrSearchEngine")
        print("  - –ù–∞–ª–∏—á–∏–µ –∏–Ω–¥–µ–∫—Å–∞ 'habr_articles' –≤ Elasticsearch")
        print("  - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: pandas, openpyxl")


if __name__ == "__main__":
    main()