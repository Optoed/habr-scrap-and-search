import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
import re

def prepare_data(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
    df['title_clean'] = df['title'].fillna('').astype(str)
    df['tags_clean'] = df['tags'].fillna('').astype(str)
    df['hubs_clean'] = df['hubs'].fillna('').astype(str)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['combined_features'] = (
            df['title_clean'] + ' ' +
            df['tags_clean'] + ' ' +
            df['hubs_clean']
    )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['title_length'] = df['title_clean'].str.len()
    df['tags_count'] = df['tags_clean'].str.count(',') + 1

    return df


def train_logistic_regression(df):
    """–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""

    print("üéØ –û–ë–£–ß–ï–ù–ò–ï –õ–û–ì–ò–°–¢–ò–ß–ï–°–ö–û–ô –†–ï–ì–†–ï–°–°–ò–ò")
    print("=" * 50)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = prepare_data(df)

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    X = df['combined_features']
    y = df['relevance']

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω: –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è + –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    model = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=1000,
            stop_words=['—Ç–µ–≥–∏', 'tags', 'hubs', 'blog', '–±–ª–æ–≥'],
            min_df=2,
            ngram_range=(1, 2)  # —É—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –ø–∞—Ä—ã —Å–ª–æ–≤
        )),
        ('classifier', LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced'  # –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã (1 –∏ 0)
        ))
    ])

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("üìö –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    model.fit(X_train, y_train)

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.3f}")
    print("\nüìä –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    print(classification_report(y_test, y_pred))

    return model, X_train, X_test, y_train, y_test


def analyze_feature_importance(model, feature_names, top_n=20):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

    print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
    print("=" * 50)

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏
    coefficients = model.named_steps['classifier'].coef_[0]
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': coefficients
    })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–ø–æ –º–æ–¥—É–ª—é)
    feature_importance['abs_importance'] = np.abs(feature_importance['importance'])
    top_features = feature_importance.sort_values('abs_importance', ascending=False).head(top_n)

    for _, row in top_features.iterrows():
        effect = "–£–í–ï–õ–ò–ß–ò–í–ê–ï–¢" if row['importance'] > 0 else "–£–ú–ï–ù–¨–®–ê–ï–¢"
        print(f"{row['feature']:<30} {effect} –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–≤–µ—Å: {row['importance']:.3f})")


def predict_new_articles(model, new_articles_df):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π"""

    print("\nüéØ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –î–õ–Ø –ù–û–í–´–• –°–¢–ê–¢–ï–ô")
    print("=" * 50)

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    new_df = prepare_data(new_articles_df)

    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    probabilities = model.predict_proba(new_df['combined_features'])[:, 1]
    new_df['predicted_relevance'] = model.predict(new_df['combined_features'])
    new_df['relevance_probability'] = probabilities

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    sorted_df = new_df.sort_values('relevance_probability', ascending=False)

    print("–¢–æ–ø-5 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:")
    for i, (idx, row) in enumerate(sorted_df.head().iterrows(), 1):
        print(f"{i}. {row['title'][:60]}... (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {row['relevance_probability']:.3f})")

    return new_df


def main():
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_excel("habr_serp_data_llm_with_relevance.xlsx")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if 'relevance' not in df.columns or df['relevance'].isna().all():
        print("‚ùå –ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–ª–æ–Ω–∫–µ 'relevance'")
        return

    print(f"üìä –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(df)}")
    print(f"üî∏ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(df[df['relevance'] == 1])}")
    print(f"üîπ –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(df[df['relevance'] == 0])}")

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    model, X_train, X_test, y_train, y_test = train_logistic_regression(df)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    feature_names = model.named_steps['tfidf'].get_feature_names_out()
    analyze_feature_importance(model, feature_names)

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüé™ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò:")
    print("=" * 50)

    test_predictions = model.predict(X_test)
    test_probs = model.predict_proba(X_test)[:, 1]

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
    for i in range(min(5, len(X_test))):
        actual = y_test.iloc[i]
        predicted = test_predictions[i]
        prob = test_probs[i]

        print(f"\n–°—Ç–∞—Ç—å—è: {X_test.iloc[i][:80]}...")
        print(f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {actual}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è: {predicted}")
        print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {prob:.3f}")
        print("‚úÖ –í–µ—Ä–Ω–æ" if actual == predicted else "‚ùå –û—à–∏–±–∫–∞")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import joblib
    joblib.dump(model, 'relevance_classifier.pkl')
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'relevance_classifier.pkl'")


if __name__ == "__main__":
    main()